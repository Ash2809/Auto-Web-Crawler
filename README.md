
# ğŸŒ Multimodal Internet Crawler + Question Answerer

A powerful multimodal system that **scrapes the web** to answer complex user queries using **text, images, and videos**. This project combines state-of-the-art AI models for captioning, transcription, retrieval, and summarization â€” producing structured, step-by-step guides with rich media.

---


## ğŸ§  Example Query

> **"Show me how to do a deadlift"**

âœ… Google Search  
âœ… Download and caption images  
âœ… Transcribe tutorial YouTube videos  
âœ… Extract and summarize instructional steps  
âœ… Output a final guide with:  
- âœ… Text answer  
- ğŸ–¼ï¸ Diagrams  
- ğŸï¸ Video summary  

---

## ğŸš€ Features

- ğŸ” **Web Crawler** (Google, YouTube)
- ğŸ–¼ï¸ **Image Captioning** (BLIP-2 / DINOv2)
- ğŸ§ **Video Transcription** (OpenAI Whisper)
- ğŸ“š **Text Summarization & RAG** (LlamaIndex / LangChain)
- ğŸ§  **Multimodal Ranking** (CLIP/Text/Temporal)
- ğŸ“˜ **Guide Generator** (Structured JSON/HTML)

---

## ğŸ§± Architecture

```
Query â†’ Scraper â†’ Media/Text Processor â†’ Multimodal Ranker â†’ Structured Guide
           |            |                        |
         Google       BLIP/Whisper            Fusion
         YouTube       + RAG Index         (Text + Visual)
```

---

## ğŸ—‚ï¸ Project Structure

```
multimodal_qa/
â”œâ”€â”€ app.py                          # Main pipeline orchestrator
â”œâ”€â”€ crawler/
â”‚   â””â”€â”€ web_scraper.py              # Google + YouTube scraping & media downloading
â”œâ”€â”€ processing/
â”‚   â”œâ”€â”€ image_captioning.py         # BLIP-2 for image captions
â”‚   â”œâ”€â”€ video_processor.py          # Whisper for video/audio
â”‚   â””â”€â”€ text_processor.py           # LangChain / LlamaIndex for RAG
â”œâ”€â”€ aligner/
â”‚   â””â”€â”€ multimodal_ranker.py        # Align visual + textual info
â”œâ”€â”€ generator/
â”‚   â””â”€â”€ guide_formatter.py          # Structured guide output
â”œâ”€â”€ models/
â”‚   â””â”€â”€ load_models.py              # Shared model loading logic
â”œâ”€â”€ data/                           # Media, transcripts, captions, outputs
â””â”€â”€ requirements.txt
```

---

## ğŸ‘¥ Team Responsibilities

| Member         | Responsibilities                                                   |
|----------------|--------------------------------------------------------------------|
| **Aashutosh** | Pipeline orchestration, RAG integration, multimodal alignment, output formatting |
| **Qusai**     | Web crawler, Google/YouTube scraping, image/video downloading      |
| **Areef**     | Image captioning (BLIP-2), video transcription (Whisper), RAG indexing |

---

## ğŸ§ª Setup Instructions

### 1. Clone the Repo

```bash
git clone https://github.com/yourusername/multimodal-qa.git
cd multimodal-qa
```

### 2. Install Dependencies

```bash
pip install -r requirements.txt
playwright install
```

### 3. Add Your API Keys

Set your OpenAI API Key:

```bash
export OPENAI_API_KEY="your-key"
```

(Optional) Create a `.env` file to store credentials.

---

## â–¶ï¸ Run the System

```bash
python app.py
```

This will:
- Scrape Google/YouTube for your query
- Caption images and transcribe videos
- Summarize all results into a structured multimodal guide

---

## ğŸ“Œ TODO (Milestones)

- [] Web crawler with Playwright
- [] BLIP-2 captioning
- [] Whisper transcription
- [] LlamaIndex or LangChain RAG
- [] Multimodal ranking with CLIP
- [] Visual-text temporal alignment
- [] Streamlit or React front-end

---

## ğŸ“„ License

MIT License â€“ open for academic or research use.

---

## ğŸ¤– Built With

- [Playwright](https://playwright.dev/)
- [Transformers](https://huggingface.co)
- [OpenAI Whisper](https://github.com/openai/whisper)
- [LangChain](https://github.com/langchain-ai/langchain)
- [LlamaIndex](https://www.llamaindex.ai/)
- [BLIP-2](https://github.com/salesforce/LAVIS)

---

## ğŸ™Œ Acknowledgements

Inspired by the latest advances in **multimodal generative AI** and RAG pipelines.
